{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of chatbot_4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO1YG7clNUDc4Ni4uwwvnPt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhishek203/E-Abhishek/blob/master/Copy_of_chatbot_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hec-Wv753dkz",
        "colab_type": "code",
        "outputId": "94e93c02-029e-4cbc-9c5c-0cbd076ee549",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "import tensorflow as tf\n",
        "import re\n",
        "import time\n",
        "import io\n",
        "import os\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import plot_model\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense, Embedding, Bidirectional, concatenate\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYBAQZHY4dV2",
        "colab_type": "code",
        "outputId": "088e0888-8a34-4a8d-924c-35217fee5764",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "path_to_zip = tf.keras.utils.get_file(\n",
        "    'cornell movie-dialogs corpus.zip', origin='http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip',\n",
        "    extract=True)\n",
        "\n",
        "path_to_lines = os.path.dirname(path_to_zip)+\"/cornell movie-dialogs corpus/movie_lines.txt\"\n",
        "path_to_conv = os.path.dirname(path_to_zip)+\"/cornell movie-dialogs corpus/movie_conversations.txt\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\n",
            "9920512/9916637 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkT2CQjs4psK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lines = io.open(path_to_lines,encoding = 'utf-8',errors='ignore').read().strip().split('\\n')\n",
        "conv_lines = io.open(path_to_conv,errors = 'ignore').read().strip().split('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKq9gYl0D3k1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "id2line ={}\n",
        "for line in lines:\n",
        "  _line = line.split(' +++$+++ ')\n",
        "  if len(_line) ==5:\n",
        "      id2line[_line[0]] = _line[4]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmjNy5cOfucK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "convs = []\n",
        "for line in conv_lines:\n",
        "    _line = line.split(' +++$+++ ')[-1][1:-1].replace(r\"'\",\"\").replace(\" \",\"\")\n",
        "    convs.append(_line.split(','))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYVsURI-tTFN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#question and answer\n",
        "question = []\n",
        "answer = []\n",
        "for k in convs:\n",
        "  for l in range(len(k)-1):\n",
        "    question.append(id2line[k[l]])\n",
        "    answer.append(id2line[k[l+1]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1NL_xcTDaMf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_text(text):\n",
        "    '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
        "\n",
        "    text = text.lower()\n",
        "    \n",
        "    text = re.sub(r\"i'm\", \"i am\", text)\n",
        "    text = re.sub(r\"he's\", \"he is\", text)\n",
        "    text = re.sub(r\"she's\", \"she is\", text)\n",
        "    text = re.sub(r\"it's\", \"it is\", text)\n",
        "    text = re.sub(r\"that's\", \"that is\", text)\n",
        "    text = re.sub(r\"what's\", \"that is\", text)\n",
        "    text = re.sub(r\"where's\", \"where is\", text)\n",
        "    text = re.sub(r\"how's\", \"how is\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"won't\", \"will not\", text)\n",
        "    text = re.sub(r\"can't\", \"cannot\", text)\n",
        "    text = re.sub(r\"n't\", \" not\", text)\n",
        "    text = re.sub(r\"n'\", \"ng\", text)\n",
        "    text = re.sub(r\"'bout\", \"about\", text)\n",
        "    text = re.sub(r\"'til\", \"until\", text)\n",
        "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
        "    text = \" \".join(text.split())\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ababvqSgD44A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clean_questions = []\n",
        "for aquestion in question:\n",
        "    clean_questions.append(clean_text(aquestion))\n",
        "    \n",
        "clean_answers = []    \n",
        "for aanswer in answer:\n",
        "    clean_answers.append(clean_text(aanswer))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJoJ8Ca1UJhg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "min_length = 2\n",
        "max_length =20\n",
        "l = 0\n",
        "short_question_temp = []\n",
        "short_ans_temp = []\n",
        "for aquestion in clean_questions:\n",
        "  if len(aquestion)>=min_length and len(aquestion)<=max_length:\n",
        "    short_question_temp.append(aquestion)\n",
        "    short_ans_temp.append(clean_answers[l])\n",
        "  l+=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qX7VW2nXt5MV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "short_ans = []\n",
        "short_question = []\n",
        "m = 0\n",
        "for aanswer in short_ans_temp:\n",
        "  if len(aanswer)>=min_length and len(aanswer)<=max_length:\n",
        "    short_ans.append(aanswer)\n",
        "    short_question.append(short_question_temp[m])\n",
        "  m+=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgByV_h7-p1G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(short_question)):\n",
        "  short_question[i]='<start> '+ short_question[i] +' <end>'\n",
        "for i in range(len(short_ans)):\n",
        "  short_ans[i] = '<start> '+ short_ans[i] +' <end>'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_HLqsq8-rjv",
        "colab_type": "code",
        "outputId": "782ceb09-89ae-4819-cecc-949ee8eb7d09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "short_ans[90]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> it is okay <end>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfdHN90NlCE0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#tokenizer\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='',oov_token='<oov>')\n",
        "\n",
        "tokenizer.fit_on_texts(short_question)\n",
        "question_seq = tokenizer.texts_to_sequences(short_question)\n",
        "question_seq = tf.keras.preprocessing.sequence.pad_sequences(question_seq,padding='post')\n",
        "tokenizer.fit_on_texts(short_ans)\n",
        "ans_seq = tokenizer.texts_to_sequences(short_ans)\n",
        "ans_seq = tf.keras.preprocessing.sequence.pad_sequences(ans_seq, padding = 'post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ckOzKZh0Xrc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len_input_seq = len(question_seq[0])\n",
        "len_output_seq = len(ans_seq[0])\n",
        "\n",
        "\n",
        "BUFFER_SIZE = len(question_seq)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(question_seq)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(tokenizer.word_index)+1\n",
        "vocab_tar_size = len(tokenizer.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((question_seq, ans_seq)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73_BMtzDh8_A",
        "colab_type": "code",
        "outputId": "e64449a4-5ff3-4390-ec23-022c13677c85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(question_seq)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22928"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyThEksG7lJz",
        "colab_type": "code",
        "outputId": "e71219f2-2c92-414c-eeba-41a212b88e6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 9]), TensorShape([64, 9]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yJ1Gvk68Awd",
        "colab_type": "code",
        "outputId": "1bf69a23-933f-438d-ccb8-730373198e8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tokenizer.word_index['<start>']"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unHunx4OF4hT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zEGxdFAGYib",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # query hidden state shape == (batch_size, hidden size)\n",
        "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # values shape == (batch_size, max_len, hidden size)\n",
        "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBDduEF2GZ4s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # used for attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kz41iOkTfp_b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69T8VQaAgMT3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xL0SL4SCgPrg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzpOeHIFgUis",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ,enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp,enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThuvGxtXgujs",
        "colab_type": "code",
        "outputId": "287e22da-ab1c-4d6f-9e16-5e44edfe0139",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "EPOCHS = 25\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 3.7410\n",
            "Epoch 1 Batch 100 Loss 2.1437\n",
            "Epoch 1 Batch 200 Loss 1.6774\n",
            "Epoch 1 Batch 300 Loss 1.8381\n",
            "Epoch 1 Loss 1.8856\n",
            "Time taken for 1 epoch 59.572468996047974 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.5407\n",
            "Epoch 2 Batch 100 Loss 1.6865\n",
            "Epoch 2 Batch 200 Loss 1.6344\n",
            "Epoch 2 Batch 300 Loss 1.7059\n",
            "Epoch 2 Loss 1.6464\n",
            "Time taken for 1 epoch 48.40249466896057 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.4179\n",
            "Epoch 3 Batch 100 Loss 1.5172\n",
            "Epoch 3 Batch 200 Loss 1.5465\n",
            "Epoch 3 Batch 300 Loss 1.6146\n",
            "Epoch 3 Loss 1.5522\n",
            "Time taken for 1 epoch 47.71618318557739 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.3455\n",
            "Epoch 4 Batch 100 Loss 1.3744\n",
            "Epoch 4 Batch 200 Loss 1.3834\n",
            "Epoch 4 Batch 300 Loss 1.4916\n",
            "Epoch 4 Loss 1.4709\n",
            "Time taken for 1 epoch 48.356080055236816 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.3714\n",
            "Epoch 5 Batch 100 Loss 1.3762\n",
            "Epoch 5 Batch 200 Loss 1.3749\n",
            "Epoch 5 Batch 300 Loss 1.3524\n",
            "Epoch 5 Loss 1.3860\n",
            "Time taken for 1 epoch 47.53962159156799 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.3426\n",
            "Epoch 6 Batch 100 Loss 1.3712\n",
            "Epoch 6 Batch 200 Loss 1.3260\n",
            "Epoch 6 Batch 300 Loss 1.3936\n",
            "Epoch 6 Loss 1.2929\n",
            "Time taken for 1 epoch 48.2018346786499 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 1.3285\n",
            "Epoch 7 Batch 100 Loss 1.3463\n",
            "Epoch 7 Batch 200 Loss 1.1378\n",
            "Epoch 7 Batch 300 Loss 1.0705\n",
            "Epoch 7 Loss 1.1922\n",
            "Time taken for 1 epoch 48.35141468048096 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.9373\n",
            "Epoch 8 Batch 100 Loss 1.0082\n",
            "Epoch 8 Batch 200 Loss 1.1706\n",
            "Epoch 8 Batch 300 Loss 1.1811\n",
            "Epoch 8 Loss 1.0806\n",
            "Time taken for 1 epoch 48.70936632156372 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.9314\n",
            "Epoch 9 Batch 100 Loss 0.9589\n",
            "Epoch 9 Batch 200 Loss 0.9014\n",
            "Epoch 9 Batch 300 Loss 1.0729\n",
            "Epoch 9 Loss 0.9657\n",
            "Time taken for 1 epoch 48.324559926986694 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.7515\n",
            "Epoch 10 Batch 100 Loss 0.7773\n",
            "Epoch 10 Batch 200 Loss 0.7370\n",
            "Epoch 10 Batch 300 Loss 0.8691\n",
            "Epoch 10 Loss 0.8502\n",
            "Time taken for 1 epoch 48.91409754753113 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.6601\n",
            "Epoch 11 Batch 100 Loss 0.7541\n",
            "Epoch 11 Batch 200 Loss 0.6839\n",
            "Epoch 11 Batch 300 Loss 0.6896\n",
            "Epoch 11 Loss 0.7413\n",
            "Time taken for 1 epoch 48.192209243774414 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.5801\n",
            "Epoch 12 Batch 100 Loss 0.5852\n",
            "Epoch 12 Batch 200 Loss 0.7437\n",
            "Epoch 12 Batch 300 Loss 0.4992\n",
            "Epoch 12 Loss 0.6427\n",
            "Time taken for 1 epoch 48.590567111968994 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.5686\n",
            "Epoch 13 Batch 100 Loss 0.4443\n",
            "Epoch 13 Batch 200 Loss 0.6077\n",
            "Epoch 13 Batch 300 Loss 0.4853\n",
            "Epoch 13 Loss 0.5540\n",
            "Time taken for 1 epoch 48.15639662742615 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.4659\n",
            "Epoch 14 Batch 100 Loss 0.4374\n",
            "Epoch 14 Batch 200 Loss 0.5200\n",
            "Epoch 14 Batch 300 Loss 0.5107\n",
            "Epoch 14 Loss 0.4734\n",
            "Time taken for 1 epoch 48.81034183502197 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.4072\n",
            "Epoch 15 Batch 100 Loss 0.4859\n",
            "Epoch 15 Batch 200 Loss 0.4120\n",
            "Epoch 15 Batch 300 Loss 0.4832\n",
            "Epoch 15 Loss 0.4077\n",
            "Time taken for 1 epoch 48.153888463974 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.3943\n",
            "Epoch 16 Batch 100 Loss 0.3606\n",
            "Epoch 16 Batch 200 Loss 0.4388\n",
            "Epoch 16 Batch 300 Loss 0.3397\n",
            "Epoch 16 Loss 0.3536\n",
            "Time taken for 1 epoch 48.38434553146362 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.2542\n",
            "Epoch 17 Batch 100 Loss 0.2557\n",
            "Epoch 17 Batch 200 Loss 0.2829\n",
            "Epoch 17 Batch 300 Loss 0.3257\n",
            "Epoch 17 Loss 0.3122\n",
            "Time taken for 1 epoch 47.6725115776062 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.2193\n",
            "Epoch 18 Batch 100 Loss 0.2880\n",
            "Epoch 18 Batch 200 Loss 0.2449\n",
            "Epoch 18 Batch 300 Loss 0.3539\n",
            "Epoch 18 Loss 0.2840\n",
            "Time taken for 1 epoch 48.388253688812256 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.1468\n",
            "Epoch 19 Batch 100 Loss 0.3009\n",
            "Epoch 19 Batch 200 Loss 0.2046\n",
            "Epoch 19 Batch 300 Loss 0.3022\n",
            "Epoch 19 Loss 0.2631\n",
            "Time taken for 1 epoch 47.703256368637085 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.2159\n",
            "Epoch 20 Batch 100 Loss 0.2289\n",
            "Epoch 20 Batch 200 Loss 0.2914\n",
            "Epoch 20 Batch 300 Loss 0.1856\n",
            "Epoch 20 Loss 0.2479\n",
            "Time taken for 1 epoch 48.431917905807495 sec\n",
            "\n",
            "Epoch 21 Batch 0 Loss 0.1948\n",
            "Epoch 21 Batch 100 Loss 0.2049\n",
            "Epoch 21 Batch 200 Loss 0.1720\n",
            "Epoch 21 Batch 300 Loss 0.2190\n",
            "Epoch 21 Loss 0.2347\n",
            "Time taken for 1 epoch 47.910574197769165 sec\n",
            "\n",
            "Epoch 22 Batch 0 Loss 0.1509\n",
            "Epoch 22 Batch 100 Loss 0.1824\n",
            "Epoch 22 Batch 200 Loss 0.2161\n",
            "Epoch 22 Batch 300 Loss 0.2236\n",
            "Epoch 22 Loss 0.2271\n",
            "Time taken for 1 epoch 48.31373834609985 sec\n",
            "\n",
            "Epoch 23 Batch 0 Loss 0.2250\n",
            "Epoch 23 Batch 100 Loss 0.2625\n",
            "Epoch 23 Batch 200 Loss 0.1987\n",
            "Epoch 23 Batch 300 Loss 0.2889\n",
            "Epoch 23 Loss 0.2214\n",
            "Time taken for 1 epoch 47.653470516204834 sec\n",
            "\n",
            "Epoch 24 Batch 0 Loss 0.1763\n",
            "Epoch 24 Batch 100 Loss 0.2877\n",
            "Epoch 24 Batch 200 Loss 0.2070\n",
            "Epoch 24 Batch 300 Loss 0.2276\n",
            "Epoch 24 Loss 0.2178\n",
            "Time taken for 1 epoch 48.38729739189148 sec\n",
            "\n",
            "Epoch 25 Batch 0 Loss 0.1832\n",
            "Epoch 25 Batch 100 Loss 0.2397\n",
            "Epoch 25 Batch 200 Loss 0.2063\n",
            "Epoch 25 Batch 300 Loss 0.2487\n",
            "Epoch 25 Loss 0.2131\n",
            "Time taken for 1 epoch 47.76413536071777 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQmY5SN1mMZN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(sentence):\n",
        "\n",
        "  sentence = clean_text(sentence)\n",
        "  sentence = '<start> ' + sentence + ' <end>'\n",
        "  tokenizer.fit_on_texts(sentence)\n",
        "\n",
        "  inputs = [tokenizer.word_index[i] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=len_input_seq,\n",
        "                                                         padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  result = ''\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(inputs,hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
        "\n",
        "  for t in range(len_output_seq):\n",
        "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                         dec_hidden,\n",
        "                                                         enc_out)\n",
        "\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "    result += tokenizer.index_word[predicted_id] + ' '\n",
        "\n",
        "    if tokenizer.index_word[predicted_id] == '<end>':\n",
        "      return result, sentence\n",
        "\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkJUUY8Jm_tA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def speak(sentence):\n",
        "  result, sentence = evaluate(sentence)\n",
        "\n",
        "  print('Input: %s' % (sentence))\n",
        "  print('Answer: {}'.format(result))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YueiYPj9nMNf",
        "colab_type": "code",
        "outputId": "fa6e4d49-d244-45e7-c39c-a17cea1ec122",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f79e8ca7048>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7hF_Fc2oMcw",
        "colab_type": "code",
        "outputId": "b98a3efe-435d-41b9-977a-4c13a39356d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "speak('the weather is very hot today')"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> the weather is very hot today <end>\n",
            "Answer: you are the room <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haG7yPkyoyn6",
        "colab_type": "code",
        "outputId": "a3682691-370f-4c33-b5ff-b94cddb62869",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "speak('why are you')"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> why are you <end>\n",
            "Answer: you say <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c06Gk6hupJVh",
        "colab_type": "code",
        "outputId": "6b3b2554-6700-4282-993a-f6e07c10ba3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "speak('where is he')"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> where is he <end>\n",
            "Answer: i do not know <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yKKNu1dpUGl",
        "colab_type": "code",
        "outputId": "c510fa47-af49-4857-822e-182ca40adb49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "speak('where is she')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> where is she <end>\n",
            "Answer: i have got the second <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJRUfQfspewX",
        "colab_type": "code",
        "outputId": "9fae25c8-da40-44aa-efee-9354f4d78f28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "speak('you want to go out')"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> you want to go out <end>\n",
            "Answer: i got you see you got you <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBNEpH6Dp8Zi",
        "colab_type": "code",
        "outputId": "4c9d8d58-9f45-48d8-b152-9d647b1666a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "speak('do you know someone from us')"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> do you know someone from us <end>\n",
            "Answer: no <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5riXFbEtZIZ",
        "colab_type": "code",
        "outputId": "2cc72490-02de-4e82-9323-756fb4551f10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "speak('how are you')"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> how are you <end>\n",
            "Answer: no <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WlwRmZnteeb",
        "colab_type": "code",
        "outputId": "ca446fe9-0389-4316-9a76-74cfa927550e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "speak('do you know english')"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> do you know english <end>\n",
            "Answer: i do not know <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHfiw2UXtknw",
        "colab_type": "code",
        "outputId": "d16395c5-6b00-4086-c038-38d1356d8618",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "speak('how were you made')"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> how were you made <end>\n",
            "Answer: i am son <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLCG3HUrtr48",
        "colab_type": "code",
        "outputId": "e2ddbe2f-682b-4da5-dedb-d7fa989fd709",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "speak('who made you')"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> who made you <end>\n",
            "Answer: yes <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0902dGGdtue9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "094144f1-c721-4f3b-d66e-bcc37acb5b53"
      },
      "source": [
        "speak('what is your name')"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> what is your name <end>\n",
            "Answer: i cannot do it <end> \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}